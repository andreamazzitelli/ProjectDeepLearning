{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "45ssSUDoqHpJ"
      },
      "source": [
        "#Introduction\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1wHPGH00jiXZUCCsaCI6dreoZdNqTyijK?usp=share_link\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"google colab logo\" height=35px></a>\n",
        "\n",
        "Check out the [pdf report](https://github.com/andrear632/ProjectDeepLearning/blob/main/Report_Project_Deep_Learning.pdf), our [GitHub repository](https://github.com/andrear632/ProjectDeepLearning) and [WandB interactive report](https://api.wandb.ai/links/project_dl/dhyw7j7v)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXwExihGrDpu"
      },
      "source": [
        "This is the colab for the Deep Learning project in which we try several models to answer mathematical questions.\n",
        "\n",
        "The first thing we did was read the dataset paper [1] in which the dataset is described. This allowed us to better understand the end goal of the authors and helped us formulate some ideas on how to approach the task.\n",
        "\n",
        "We decided to implement some of the models discussed in the paper; in this way, we would have a baseline to compare with our implementation. In particular, we thought that it would be interesting to compare the results obtained by an LSTM model and a Transformer model because they both can work with sequence-to-sequence tasks but are based on different strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCIfLPRkqW9G"
      },
      "source": [
        "Before starting the analysis of the selected models, we should mention some limitations that influenced our work. Google Colab’s free tier does not provide much computational power; this highly limited how many samples we were able to use and for how long we were able to train/test the\n",
        "models. Indeed, after some time of using the free GPU, the account that uses it would be temporarily suspended for exceeding the GPU availability time (circa four hours).\n",
        "\n",
        "This means that we had to choose a relevant subset of the dataset to not exceed the available resources and we were unable to train as long as needed to reach the accuracies of the papers of the dataset [1] and the SOTA\n",
        "[2]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh5rcecfIyC_"
      },
      "source": [
        "# Imports\n",
        "In this section we import all the needed libraries to run our models and we set some useful variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLzRPjd8qCc8"
      },
      "outputs": [],
      "source": [
        "pip install wget pytorch_lightning datasets wandb --quiet "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_skG4-RfpQ0W"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade gdown --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmjt25b-Dz7M"
      },
      "outputs": [],
      "source": [
        "# Importing dependencies\n",
        "import json\n",
        "import os\n",
        "import wget\n",
        "import random\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from pytorch_lightning import Trainer, LightningDataModule, LightningModule\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEHQ2aOfqUO4"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy0hBvR5Emn9"
      },
      "source": [
        "# Dataset and Preprocessing\n",
        "The following class is used to handle all the operations to retrieve and prepare the dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hwRwuFyRq5QT"
      },
      "source": [
        "We use this class to generate four dataloaders: one dataloader for the training phase, another one for the validation phase and two dataloaders for the testing phase (interpolate and extrapolate).\n",
        "\n",
        "To download the data and create the dataset we use the load_dataset function from Hugging Face together with custom loading scripts in which we define the data source and the splits (they can be found in [our GitHub repository](https://github.com/andrear632/ProjectDeepLearning)).\n",
        "\n",
        "In this class we also define a method that extracts relevant information from the dataset: the dictionary containing all possible characters and their corresponding indexes, the maximum length of questions and the maximum length of answers.\n",
        "\n",
        "Using these metadata we encode the dataset so that it can be used by the models. We also provide a method to encode new questions not present in the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "824SLlDAElT3"
      },
      "outputs": [],
      "source": [
        "class MathDataModule(LightningDataModule):\n",
        "  def __init__(self, seed: int = 16, batch_size: int = 1024, workers: int = 2, chosen_dataset: int = 0):  # 12 per colab pro, 2 per colab free\n",
        "    super().__init__()\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "    # '&' is the padding character,\n",
        "    # '#' is the start of sentence char,\n",
        "    # '@' is the end of sentence char\n",
        "    self.char_to_idx = {\"&\": 0, \"#\": 1, \"@\": 2}\n",
        "    self.question_max_length = 0\n",
        "    self.answer_max_length = 0\n",
        "    self.manual_seed = seed\n",
        "    self.workers = workers\n",
        "\n",
        "    if chosen_dataset == 0:\n",
        "      # only round, 10000 samples per module\n",
        "      self.url = \"https://raw.githubusercontent.com/andrear632/ProjectDeepLearning/main/loading_script_round_10000.py\"\n",
        "    elif chosen_dataset == 1:\n",
        "      # only round, 2000000 samples per module\n",
        "      self.url = \"https://raw.githubusercontent.com/andrear632/ProjectDeepLearning/main/loading_script_round_2000000.py\"\n",
        "    elif chosen_dataset == 2:\n",
        "      # only round + composed, 2000000 samples per module\n",
        "      self.url = \"https://raw.githubusercontent.com/andrear632/ProjectDeepLearning/main/loading_script_round+composed_2000000.py\"\n",
        "    elif chosen_dataset == 3:\n",
        "      # only differentiate + round, 2000000 samples per module\n",
        "      self.url = \"https://raw.githubusercontent.com/andrear632/ProjectDeepLearning/main/loading_script_differentiate+round_2000000.py\"\n",
        "    elif chosen_dataset == 4:\n",
        "      # only differentiate + round + evaluate, 2000000 samples per module\n",
        "      self.url = \"https://raw.githubusercontent.com/andrear632/ProjectDeepLearning/main/loading_script_differentiate+round+evaluate_2000000.py\"\n",
        "\n",
        "  # downloading the dataset and extracting relevant informations\n",
        "  def prepare_data(self): \n",
        "    wget.download(self.url, \"/content/loading_script.py\")\n",
        "    load_dataset('/content/loading_script.py', download_mode=\"force_redownload\")\n",
        "    self._extract_metadata(\"/content/jsondataset/\")\n",
        "  \n",
        "  def setup(self, stage: str):\n",
        "\n",
        "    if stage == \"fit\":  # setup dataset for train and validations phases\n",
        "      \n",
        "      # Loading train split and encoding the samples into indexes\n",
        "      dataset = load_dataset('/content/loading_script.py', split=\"train\").map(self._encode)\n",
        "      dataset.set_format(type=\"torch\")\n",
        "\n",
        "      # Splitting into train and validation sets\n",
        "      train_len = dataset.num_rows\n",
        "      split = [int(train_len*0.9), train_len - int(train_len*0.9)]\n",
        "      self.dataset_train, self.dataset_val = random_split(dataset, split, generator=torch.Generator().manual_seed(self.manual_seed))\n",
        "\n",
        "    if stage == \"test\":  # setup dataset for test phase\n",
        "\n",
        "      # Loading test splits and encoding the samples into indexes\n",
        "      interpolate = load_dataset('/content/loading_script.py', split=\"interpolate\").map(self._encode)\n",
        "      extrapolate = load_dataset('/content/loading_script.py', split=\"extrapolate\").map(self._encode)\n",
        "\n",
        "      interpolate.set_format(type=\"torch\")\n",
        "      extrapolate.set_format(type=\"torch\")\n",
        "\n",
        "      self.dataset_interpolate = interpolate\n",
        "      self.dataset_extrapolate = extrapolate\n",
        "  \n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True, num_workers=self.workers)\n",
        "  \n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.dataset_val, batch_size=self.batch_size, drop_last=True, num_workers=self.workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    dl1 = DataLoader(self.dataset_interpolate, batch_size=self.batch_size, drop_last=True, num_workers=self.workers)\n",
        "    dl2 = DataLoader(self.dataset_extrapolate, batch_size=self.batch_size, drop_last=True, num_workers=self.workers)\n",
        "    return [dl1, dl2]\n",
        "\n",
        "  # Function that extracts information from the dataset (question_max_length, answer_max_length) and builds the dictionary\n",
        "  def _extract_metadata(self, dir_path): \n",
        "\n",
        "    for folder in [\"interpolate\", \"extrapolate\", \"train\"]:\n",
        "      folder_path = dir_path + folder + \"/\"\n",
        "\n",
        "      for file_name in os.listdir(folder_path):\n",
        "        with open(folder_path + file_name, \"r\") as input_file:\n",
        "\n",
        "          for line_no, line in enumerate(input_file):\n",
        "            line = line.strip()\n",
        "\n",
        "            for char in line:  # populating dictionary\n",
        "              if char not in self.char_to_idx:\n",
        "                self.char_to_idx[char] = len(self.char_to_idx)\n",
        "            \n",
        "            line = json.loads(line)\n",
        "\n",
        "            if line_no % 2 != 0:  # analysing answer line\n",
        "              if len(line[\"answer\"]) > self.answer_max_length:\n",
        "                self.answer_max_length = len(line[\"answer\"])\n",
        "            else:  # analysing question line\n",
        "              if len(line[\"question\"]) > self.question_max_length:\n",
        "                self.question_max_length = len(line[\"question\"])\n",
        "\n",
        "          input_file.close()\n",
        "\n",
        "    self.idx_to_char = {value: key for key, value in self.char_to_idx.items()}  # building the inverse dictionary\n",
        "\n",
        "  # Function that encodes both questions and answers using the dictionary of the dataset\n",
        "  def _encode(self, input):\n",
        "\n",
        "    dictionary_for_variables = {\n",
        "        \"question\": (self.question_max_length, []),\n",
        "        \"answer\": (self.answer_max_length, [])\n",
        "    }\n",
        "\n",
        "    for key in dictionary_for_variables.keys():\n",
        "\n",
        "      string_to_turn = input[str(key)]\n",
        "      dictionary_for_variables[key][1].append(1)  # adding start of line char \n",
        "\n",
        "      for char in string_to_turn:\n",
        "\n",
        "        if char not in self.char_to_idx.keys():\n",
        "          raise Exception(\"character not found in dict\")\n",
        "\n",
        "        dictionary_for_variables[key][1].append(self.char_to_idx[char])  # turning character in index\n",
        "\n",
        "      dictionary_for_variables[key][1].append(2)  # adding end of line char\n",
        "\n",
        "      for i in range(dictionary_for_variables[key][0]-len(input[str(key)])):\n",
        "        dictionary_for_variables[key][1].append(0)  # adding padding after \n",
        "\n",
        "    return {\"question\": dictionary_for_variables[\"question\"][1], \"answer\": dictionary_for_variables[\"answer\"][1]}\n",
        "  \n",
        "  # Function to encode questions not from the dataset\n",
        "  def encode_question(self, input):\n",
        "    question = []\n",
        "    question.append(1)  # adding start of line char \n",
        "\n",
        "    for char in input:\n",
        "      if char not in self.char_to_idx.keys():\n",
        "        raise Exception(\"character \" + char + \" not found in dict\")\n",
        "      question.append(self.char_to_idx[char])  # turning character into index\n",
        "    \n",
        "    question.append(2)  # adding end of line char \n",
        "\n",
        "    for i in range(self.question_max_length-len(input)):\n",
        "        question.append(0)  # adding padding after \n",
        "\n",
        "    question = torch.tensor(question, device=device).unsqueeze(0)\n",
        "\n",
        "    return {\"question\": question}\n",
        "\n",
        "  def get_dictionaries(self):  # returns dictionaries: char_to_idx and idx_to_char\n",
        "      return (self.char_to_idx, self.idx_to_char)\n",
        "\n",
        "  def get_dictionary_size(self):  # returns length of the dictionary\n",
        "      return (len(self.char_to_idx))\n",
        "\n",
        "  def get_max_lengths(self):  # returns the maximum lengths of questions and answers\n",
        "      return (self.question_max_length + 2, self.answer_max_length + 2)\n",
        "\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnAsuqXrIfSv"
      },
      "source": [
        "# Model definitions\n",
        "Run the utilities section and the section corresponding to the model you want to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIAeNeOORtpX"
      },
      "source": [
        "## Utilities\n",
        "This section contains useful functions which will be called by the models classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g1csATFryqC"
      },
      "source": [
        "The metric that we use to assess the performance of the models is the accuracy of getting the correct answer to the question. More in detail, we followed the same criterion proposed in the dataset paper [1] in Paragraph 2.4. This means that the predicted answer is correct if and only if it matches character-for-character the correct answer. This also means that the model must be able to correctly predict the end of string token, otherwise the predicted answer would be wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez2Pdn5dbXDM"
      },
      "outputs": [],
      "source": [
        "# Function that computes the accuracy as defined in the \"mathematics dataset\" paper [1]\n",
        "def paper_accuracy(predicted_answers, correct_answers):\n",
        "  num_correct_answers = 0\n",
        "\n",
        "  # constant tokens\n",
        "  start_of_line = 1\n",
        "  end_of_line = 2\n",
        "\n",
        "  for i in range(len(predicted_answers)):\n",
        "\n",
        "    # Preparing predicted answer\n",
        "    single_predicted_answer = torch.argmax(predicted_answers[i], 1).tolist()  # vector of shape (answer_max_length) (concatenates the max value for each row)\n",
        "      \n",
        "    if (start_of_line in single_predicted_answer and end_of_line in single_predicted_answer):  # check if there are start and enf of line char\n",
        "      single_predicted_answer = single_predicted_answer[1:single_predicted_answer.index(end_of_line)]  # removing start and end of line char and additional characters\n",
        "    else:  # predicted answer is wrong\n",
        "      continue \n",
        "\n",
        "    # Preparing correct answer\n",
        "    single_correct_answer = correct_answers[i].tolist()\n",
        "    single_correct_answer = single_correct_answer[1:single_correct_answer.index(end_of_line)]  # removing start of line, end of line and following characters\n",
        "      \n",
        "    # If the predicted answer and the correct one have the same exact characters, the predicted answer is correct\n",
        "    if (single_predicted_answer == single_correct_answer):\n",
        "      num_correct_answers += 1\n",
        "\n",
        "  return num_correct_answers/len(predicted_answers)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHc903Qbg6AV"
      },
      "outputs": [],
      "source": [
        "def print_batch(dictionary, questions, res):\n",
        "  \n",
        "  questions = questions[\"question\"]\n",
        "  index_list = torch.argmax(res[0], 1) # reverse one_hot\n",
        "  question_string = answer_string = \"\"\n",
        "\n",
        "  for j in range(len(questions[0])):\n",
        "    question_string = question_string + dictionary[questions[0][j].item()]\n",
        "\n",
        "  for j in range(len(index_list)):\n",
        "    answer_string = answer_string + dictionary[index_list[j].item()]\n",
        "\n",
        "  question_string = question_string[1:question_string.index(\"@\")]\n",
        "  if \"@\" in answer_string:\n",
        "    answer_string = answer_string[1:answer_string.index(\"@\")]\n",
        "  else:\n",
        "    answer_string = \"cannot generate answer\"\n",
        "        \n",
        "  print(question_string, answer_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jurp_tFKg7I9"
      },
      "outputs": [],
      "source": [
        "def print_correct(dictionary, batch, res):\n",
        "  \n",
        "  for i in range(len(res)):\n",
        "    \n",
        "    index_list = torch.argmax(res[i], 1) # reverse one_hot\n",
        "    predicted_string = answer_string = question_string = \"\"\n",
        "\n",
        "    for j in range(len(batch[\"question\"][i])):\n",
        "      question_string = question_string + dictionary[batch[\"question\"][i][j].item()]\n",
        "\n",
        "    for j in range(len(index_list)):\n",
        "      predicted_string = predicted_string + dictionary[index_list[j].item()]\n",
        "      \n",
        "    for j in range(len(batch[\"answer\"][i])):\n",
        "      answer_string = answer_string + dictionary[batch[\"answer\"][i][j].item()]\n",
        "\n",
        "    print(question_string, predicted_string, answer_string)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-KCL0xxH84t"
      },
      "source": [
        "## LSTM\n",
        "The first model that we implement for this task is inspired by the ”Simple LSTM” presented in the dataset paper [1]. Its architecture is simple and straightforward because it is made up of a single LSTM cell. This cell sequentially processes the question and the answer, one character at a time.\n",
        "Furthermore, as suggested in the dataset paper [1], we introduced 16 ”thinking steps” before outputting the answer so that the model can correctly process the question. To do so, we pass a zero input to the LSTM to update the hidden state and the cell state.\n",
        "\n",
        "During training, we also introduced teacher forcing to improve the results. In this way, when computing a character of the answer, the model receives as input the previous correct character with a fixed probability instead of the predicted one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmY7yIZPbj3h"
      },
      "outputs": [],
      "source": [
        "class LSTM(LightningModule):\n",
        "  def __init__(self, dict_size, sizes, dictionaries, teacher_forcing_ratio: float = 0.5):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_units = 2048\n",
        "    self.question_max_length = sizes[0]\n",
        "    self.answer_max_length = sizes[1]\n",
        "    self.dict_size = dict_size\n",
        "    self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "    self.dictionary = dictionaries[1]\n",
        "\n",
        "    # Initializing used layers\n",
        "    self.lstm = nn.LSTM(self.dict_size, self.hidden_units)\n",
        "    self.linear_layer = nn.Linear(self.hidden_units, dict_size, bias=False)  # used to return vectors of shape (dict_size)\n",
        "\n",
        "  def forward(self, batch):\n",
        "\n",
        "    # Preparing the batch for processing. \n",
        "    # We need to transpose the batch from shape (batch_size, question_max_length, dict_size) to (question_max_length, batch_size, dict_size)\n",
        "    # to match the input size required by nn.LSTM\n",
        "\n",
        "    batch_size = len(batch[\"question\"])\n",
        "\n",
        "    batch_questions = batch[\"question\"]  # shape (batch_size, question_max_length)\n",
        "    batch_questions = torch.transpose(batch_questions, 0, 1)  # shape (question_max_length, batch_size)\n",
        "    batch_questions = F.one_hot(batch_questions, self.dict_size)  # shape (question_max_length, batch_size, dict_size)\n",
        "    batch_questions = batch_questions.float()\n",
        "\n",
        "    if self.training:\n",
        "      batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "      batch_answers = torch.transpose(batch_answers, 0, 1)  # shape (answer_max_length, batch_size)\n",
        "      batch_answers = F.one_hot(batch_answers, self.dict_size)  # shape (answer_max_length, batch_size, dict_size)\n",
        "      batch_answers = batch_answers.float()\n",
        "\n",
        "    # Initializing hidden_state and cell_state used by the lstm cell. In our case D = 1 (because it is not bidirectional) and num_layer = 1\n",
        "    hidden_state = torch.zeros(1, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float).to(device)  # shape (D*num_layers, batch_size, H_out)\n",
        "    cell_state = torch.zeros(1, batch_size, self.hidden_units, requires_grad=True, dtype=torch.float).to(device)  # shape (D*num_layers, batch_size, H_cell)\n",
        "\n",
        "    # Initializing result tensor\n",
        "    result = torch.empty(self.answer_max_length, batch_size, self.dict_size).to(device)  # shape (answer_max_length, batch_size, dict_size)\n",
        "\n",
        "    self.lstm.flatten_parameters()  # used to improve performance on GPU\n",
        "\n",
        "    # Generating hidden_state and cell_state representing the questions, at each iteration 'i' \n",
        "    # we feed to the lstm cell the i-th character of each question in the batch.\n",
        "    # input shape (1, batch_size, dict_size)\n",
        "    # output shape (1, batch_size, hidden_size)\n",
        "\n",
        "    for i in range(self.question_max_length):\n",
        "      output, (hidden_state, cell_state) = self.lstm(batch_questions[i].unsqueeze(0), (hidden_state, cell_state))\n",
        "      \n",
        "    # Thinking steps used to improve performance\n",
        "    think_vector = torch.zeros_like(batch_questions[0].unsqueeze(0)).float()\n",
        "\n",
        "    for i in range(16):\n",
        "      _, (hidden_state, cell_state) = self.lstm(think_vector, (hidden_state, cell_state))\n",
        "\n",
        "    # Generating answer using hidden_state and cell_state.\n",
        "    # Depending if we are in training or in evaluation we prepare the input to give the lstm cell in a different way:\n",
        "    # - training: with probability 'teacher_forcing_ratio' we feed as input the correct previous char otherwise we use the predicted one.\n",
        "    # - evaluation: the first time we pass the encoded start of line char otherwise we pass the previous predicted char\n",
        "    for i in range(self.answer_max_length):\n",
        "\n",
        "      result_temp = self.linear_layer(output[0])  # shape (batch_size, dict_size)\n",
        "      result[i] = result_temp\n",
        "\n",
        "      if self.training:\n",
        "        teacher_force = random.random() < self.teacher_forcing_ratio  # true or false with prob 'teacher_forcing_ratio'\n",
        "\n",
        "        if teacher_force:  # use correct previous char\n",
        "          input = batch_answers[i].unsqueeze(0)  # shape (1, batch_size, dict_size)\n",
        "        else:  # use previous predicted char \n",
        "          input = F.one_hot(torch.argmax(result_temp, 1), self.dict_size).unsqueeze(0).float().to(device)  # shape (1, batch_size, dict_size)\n",
        "      \n",
        "      else:\n",
        "        if i == 0:\n",
        "          input = F.one_hot(torch.tensor([1]), self.dict_size).repeat(batch_size, 1).unsqueeze(0).float().to(device)  # shape (1, batch_size, dict_size)\n",
        "        else:\n",
        "          input = F.one_hot(torch.argmax(result_temp, 1), self.dict_size).unsqueeze(0).float().to(device)  # shape (1, batch_size, dict_size)\n",
        "      \n",
        "      output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))  # shape (1, batch_size, hidden_size)\n",
        "\n",
        "    return torch.transpose((result), 0, 1)  # shape (batch_size, answer_max_length, dict_size)\n",
        "\n",
        "  def training_step(self, batch, _):\n",
        "\n",
        "    # Preparing inputs  \n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "    batch_answers = batch_answers.flatten(0, 1)  # shape (batch_size * answer_max_length)\n",
        "\n",
        "    # Computing prediction and loss\n",
        "    pred = self(batch).flatten(0, 1)  # shape (batch_size * answer_max_length, dict_size)\n",
        "    loss = F.cross_entropy(pred, batch_answers, ignore_index=0)  # 'ignore_index' allows us to ignore the padding added by the data loader\n",
        "    \n",
        "    self.log(\"train_loss\", loss)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, _):\n",
        "\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy \n",
        "    pred = self.predict(batch)  # shape (batch_size, answer_max_length, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "\n",
        "    self.log(\"val_tot_accuracy\", accuracy)  # at the end of every epoch it logs the average of the accuracies of each batch\n",
        "    return accuracy\n",
        "\n",
        "  def test_step(self, batch, batch_idx, dataset_idx):\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy \n",
        "    pred = self.predict(batch.copy())  # shape (batch_size, answer_max_length, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "    \n",
        "    if (batch_idx == 0):\n",
        "      print_correct(self.dictionary, batch, pred)\n",
        "\n",
        "    self.log(\"test_tot_accuracy\", accuracy)  # at the end of every epoch it is logged the average of the accuracies of each batch\n",
        "    \n",
        "    return accuracy\n",
        "  \n",
        "  def predict(self, questions):\n",
        "    return self(questions)\n",
        "\n",
        "  def print_predict(self, questions):\n",
        "    pred = self.predict(questions.copy())\n",
        "    print_batch(self.dictionary, questions, pred)\n",
        "    return\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=6e-4, betas=(0.9, 0.995), eps=1e-9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqNtvtuyH-dd"
      },
      "source": [
        "## Transformer\n",
        "The second model we chose to implement is a standard Transformer that follows the architecture described in the paper ”Attention Is All You Need ” (Vaswani et al.)[3]. We used common values for the hyperparameters such as 8 heads for each multi-head attention component and 6 layers for both the encoder and the decoder. We also tested the model using some dropout layers as suggested in\n",
        "the transformer paper [3] but we obtained worse results, so we decided to disable it.\n",
        "The weights of the embedding layer and the final linear layer are shared to ensure better decoding, as in the transformer paper [3]. In addition, we use sinusoidal positional encodings to make use of the order of the\n",
        "sequences. In addition to the usual mask needed in the masked multi-attention mechanism, we also implemented padding masks to ensure that the padding at the end of each question and answer is not considered when computing the attention scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4mw9e9XcSHI"
      },
      "outputs": [],
      "source": [
        "class TotalEmbeddings(LightningModule):\n",
        "\n",
        "  def __init__(self, dict_size, sizes, embedding_size, padding_idx, p=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.dict_size = dict_size\n",
        "    self.max_length = max(sizes[0], sizes[1])\n",
        "    self.embedding_size = embedding_size\n",
        "    self.padding_idx = padding_idx\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    self.scale = math.sqrt(self.embedding_size)\n",
        "\n",
        "    # Layer Initialization\n",
        "    self.embedding_layer = nn.Embedding(self.dict_size, self.embedding_size, padding_idx=self.padding_idx, device=device)\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "    \n",
        "    # Sinusoidal Positional Embeddings Computation\n",
        "    self.positional_embedding = self._initialize_pos_embedding().to(device)  # shape (max_len, embedding_size)\n",
        "\n",
        "  def forward(self, input):\n",
        "    input = input.to(device)  # shape (batch_size, input_len)\n",
        "    char_embedding = self.embedding_layer(input)*self.scale\n",
        "    return self.dropout(char_embedding + self.positional_embedding[:input.shape[1], :])  # shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "  def _initialize_pos_embedding(self):\n",
        "    positional_embeddings = torch.zeros(self.max_length, self.embedding_size)  # shape (max_len, embedding_size)\n",
        "    for pos in range(self.max_length):\n",
        "      for i in range(0, self.embedding_size, 2):\n",
        "        positional_embeddings[pos, i] = math.sin(pos / (10000 ** (i/self.embedding_size)))\n",
        "        positional_embeddings[pos, i + 1] = math.cos(pos / (10000 ** (i/self.embedding_size)))\n",
        "    return positional_embeddings  # shape (max_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69sw9H0zcxsN"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFFN(LightningModule):\n",
        "\n",
        "  def __init__(self, embedding_size, hidden_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_units = hidden_units\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.linear_1 = nn.Linear(self.embedding_size, self.hidden_units, device=device)\n",
        "    self.linear_2 = nn.Linear(self.hidden_units, self.embedding_size, device=device)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.linear_1(input)  # shape (batch_size, input_len, hidden_units)\n",
        "    output = self.relu(output)\n",
        "    output = self.linear_2(output)  # shape (batch_size, input_len, embedding_size)\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxamrOsvcyR-"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, masked=False):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.masked = masked\n",
        "    self.dk = self.dv = self.embedding_size//self.num_heads\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.attention_heads = nn.ModuleList([AttentionHead(self.dk, self.dv, self.embedding_size, self.masked) for _ in range(self.num_heads)])\n",
        "    self.Wo = nn.Linear(self.num_heads*self.dk, self.embedding_size, bias=False, device=device)  # shape (num_heads*dv, embedding_size)\n",
        "  \n",
        "  def forward(self, batch_keys, batch_queries, batch_values, padding_mask):  # input shape (batch_size, len_k/len_q/len_v, embedding_size)\n",
        "    \n",
        "    attention_heads_results = []\n",
        "    \n",
        "    for head in self.attention_heads:\n",
        "      attention_heads_results.append(head(batch_keys, batch_queries, batch_values, padding_mask))  # shape (batch_size, len_q, dv)\n",
        "    concatenated_results = torch.cat(attention_heads_results, 2)  # shape (batch_size, len_q, num_heads*dv)\n",
        "    \n",
        "    return self.Wo(concatenated_results) # shape (batch_size, len_q, embedding_size)\n",
        "\n",
        "\n",
        "# We use the same notation as Vaswani et al meaning that dk, dv are the dimensions in which we project the input to create the keys and values\n",
        "class AttentionHead(LightningModule):\n",
        "  \n",
        "  def __init__(self, dk, dv, embedding_size, masked):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.dk = dk\n",
        "    self.dv = dv\n",
        "    self.masked = masked\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.Wk = nn.Linear(embedding_size, dk, bias=False, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wq = nn.Linear(embedding_size, dk, bias=False, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wv = nn.Linear(embedding_size, dv, bias=False, device=device)  # shape (embedding_size, dv)\n",
        "    \n",
        "  \n",
        "  def forward(self, batch_keys, batch_queries, batch_values, padding_mask):  # input shape (batch_size, len_k/len_q/len_v, embedding_size)\n",
        "    \n",
        "    keys = self.Wk(batch_keys)  # shape (batch_size, len_k, dk)\n",
        "    queries = self.Wq(batch_queries)  # shape (batch_size, len_q, dk)\n",
        "    values = self.Wv(batch_values)  # shape (batch_size, len_v, dv)\n",
        "\n",
        "    unnormalized_attention_score = torch.matmul(queries, keys.transpose(1, 2)) / math.sqrt(self.dk)  #shape (batch_size, len_q, len_k)\n",
        "    \n",
        "    if self.masked:\n",
        "      input_len_1 = batch_queries.shape[1]\n",
        "      input_len_2 = batch_keys.shape[1]\n",
        "      target_mask = torch.triu(torch.full((input_len_1, input_len_2), float(\"-inf\"), device=device), diagonal=1)\n",
        "      unnormalized_attention_score = unnormalized_attention_score + target_mask\n",
        "    \n",
        "    # padding_mask shape (batch_size, len_k)\n",
        "    padding_mask = padding_mask.unsqueeze(1).repeat(1, batch_queries.shape[1], 1)  # shape (batch_size, len_q, len_k)\n",
        "    unnormalized_attention_score = unnormalized_attention_score.masked_fill_(padding_mask, float(\"-inf\"))\n",
        "\n",
        "    attention_score = F.softmax(unnormalized_attention_score, 2)\n",
        "    return torch.matmul(attention_score, values)  # shape (batch_size, len_q, dv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVcV65DxdXjH"
      },
      "outputs": [],
      "source": [
        "class Decoder(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, num_layers, p=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.layers = nn.ModuleList(\n",
        "        [DecoderLayer(self.embedding_size, self.num_heads, self.hidden_units, p=self.dropout_probability) for _ in range(self.num_layers)])\n",
        "\n",
        "  def forward(self, encoder_batch, outputs_batch, src_padding_mask, tgt_padding_mask):\n",
        "    # encoder_batch shape (batch_size, encoder_len, embedding_size)\n",
        "    # outputs_batch shape (batch_size, outputs_len, embedding_size)\n",
        "    \n",
        "    decoder_layer_ouput = outputs_batch\n",
        "\n",
        "    for decoder_layer in self.layers:\n",
        "      decoder_layer_ouput = decoder_layer(encoder_batch, decoder_layer_ouput, src_padding_mask, tgt_padding_mask)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    \n",
        "    return decoder_layer_ouput  # shape (batch_size, outputs_len, embedding_size)\n",
        "\n",
        "class DecoderLayer(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.ffn = PositionwiseFFN(self.embedding_size, self.hidden_units)\n",
        "\n",
        "    self.mha = MultiHeadAttention(self.embedding_size, self.num_heads)\n",
        "    self.masked_mha = MultiHeadAttention(self.embedding_size, self.num_heads, masked=True)\n",
        "    \n",
        "    self.layer_norm_1 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_2 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_3 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "  \n",
        "  \n",
        "  def forward(self, encoder_batch, outputs_batch, src_padding_mask, tgt_padding_mask):\n",
        "    # encoder_batch shape (batch_size, encoder_len, embedding_size)\n",
        "    # outputs_batch shape (batch_size, outputs_len, embedding_size)\n",
        "\n",
        "    masked_mha_output = self.masked_mha(outputs_batch, outputs_batch, outputs_batch, tgt_padding_mask) # shape (batch_size, outputs_len, embedding_size)\n",
        "    masked_mha_output = self.dropout(masked_mha_output)\n",
        "    masked_mha_output = self.layer_norm_1(masked_mha_output + outputs_batch)  # add & norm\n",
        "\n",
        "    mha_output = self.mha(encoder_batch, masked_mha_output, encoder_batch, src_padding_mask)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    mha_output = self.dropout(mha_output)\n",
        "    mha_output = self.layer_norm_2(mha_output + masked_mha_output)  # add & norm\n",
        "\n",
        "    ffn_output = self.ffn(mha_output)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    ffn_output = self.dropout(ffn_output)\n",
        "    ffn_output = self.layer_norm_3(ffn_output + mha_output)  # add & norm\n",
        "\n",
        "    return ffn_output  # shape (batch_size, outputs_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TVWVDYFdZS1"
      },
      "outputs": [],
      "source": [
        "class Encoder(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, num_layers, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.layers = nn.ModuleList(\n",
        "        [EncoderLayer(self.embedding_size, self.num_heads, self.hidden_units, p=self.dropout_probability) for _ in range(self.num_layers)])\n",
        "\n",
        "\n",
        "  def forward(self, batch, src_padding_mask):  # input shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "    encoder_layer_output = batch\n",
        "\n",
        "    for encoder_layer in self.layers:\n",
        "      encoder_layer_output = encoder_layer(encoder_layer_output, src_padding_mask)  # shape (batch_size, input_len, embedding_size)\n",
        "    \n",
        "    return encoder_layer_output  # shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "\n",
        "class EncoderLayer(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.ffn = PositionwiseFFN(self.embedding_size, self.hidden_units)\n",
        "    self.mha = MultiHeadAttention(self.embedding_size, self.num_heads)\n",
        "    \n",
        "    self.layer_norm_1 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_2 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "  \n",
        "  def forward(self, batch, src_padding_mask):  # input shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "    mha_output = self.mha(batch, batch, batch, src_padding_mask)  # shape (batch_size, input_len, embedding_size)\n",
        "    mha_output = self.dropout(mha_output)\n",
        "    mha_output = self.layer_norm_1(mha_output + batch)  # add & norm\n",
        "\n",
        "    ffn_output = self.ffn(mha_output)  # shape (batch_size, input_len, embedding_size)\n",
        "    ffn_output = self.dropout(ffn_output)\n",
        "    ffn_output = self.layer_norm_2(ffn_output + mha_output)  # add & norm\n",
        "\n",
        "    return ffn_output  # shape (batch_size, input_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ypb-sep4dcE-"
      },
      "outputs": [],
      "source": [
        "class Transformer(LightningModule):\n",
        "  \n",
        "  def __init__(self, dict_size, sizes, dictionaries, \n",
        "               embedding_size = 512, hidden_units = 2048, \n",
        "               num_heads = 8, num_encoder_layers=6, \n",
        "               num_decoder_layers= 6, padding_idx=0, p=0.):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.dict_size = dict_size\n",
        "    self.question_max_length = sizes[0]\n",
        "    self.answer_max_length = sizes[1]\n",
        "    self.max_len = max(sizes[0], sizes[1])\n",
        "    self.padding_idx = padding_idx\n",
        "    self.idx_to_char = dictionaries[1]  # dictionary index to char\n",
        "    self.dropout_probability = p\n",
        "    \n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_heads = num_heads\n",
        "    self.num_encoder_layers = num_encoder_layers\n",
        "    self.num_decoder_layers = num_decoder_layers\n",
        "    \n",
        "    # Layers Initialization\n",
        "    self.total_embedding = TotalEmbeddings(self.dict_size, sizes, self.embedding_size, self.padding_idx, p=self.dropout_probability)\n",
        "\n",
        "    self.encoder = Encoder(self.embedding_size, num_heads= self.num_heads, \n",
        "                           hidden_units=self.hidden_units, num_layers=self.num_encoder_layers, p=self.dropout_probability)\n",
        "    \n",
        "    self.decoder = Decoder(self.embedding_size, num_heads= self.num_heads, \n",
        "                           hidden_units=self.hidden_units, num_layers=self.num_decoder_layers, p=self.dropout_probability)\n",
        "    \n",
        "\n",
        "  def forward(self, batch):\n",
        "    \n",
        "    questions = (batch[\"question\"]).to(device)  # shape (batch_size, question_max_length)\n",
        "    answers = (batch[\"answer\"]).to(device)  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    source_pad_mask = (questions == 0).to(device)  # shape (batch_size, question_max_length)\n",
        "    target_pad_mask = (answers == 0).to(device)  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    embedded_questions = self.total_embedding(questions)  # shape (batch_size, question_max_length, embedding_size)\n",
        "    embedded_answers = self.total_embedding(answers)  # shape (batch_size, answer_max_length-1, embedding_size)\n",
        "\n",
        "    encoder_output = self.encoder(embedded_questions, src_padding_mask=source_pad_mask)  # shape (batch_size, question_max_length, embedding_size)\n",
        "    decoder_output = self.decoder(encoder_output, embedded_answers, src_padding_mask=source_pad_mask, tgt_padding_mask=target_pad_mask)  # shape (batch_size, answer_max_length-1, embedding_size)\n",
        "\n",
        "    # Softmax not used because of nn.CrossEntropyLoss\n",
        "    return torch.matmul(decoder_output, torch.transpose(self.total_embedding.embedding_layer.weight, 0, 1))  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "\n",
        "  def training_step(self, batch, _):\n",
        "    # Preparing inputs  \n",
        "    batch_answers = batch[\"answer\"][:, 1:].flatten(0, 1)  # shape (batch_size * answer_max_length-1)\n",
        "    batch[\"answer\"] = batch[\"answer\"][:, :-1]  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    # Computing prediction and loss\n",
        "    pred = self(batch).flatten(0, 1)  # shape (batch_size * answer_max_length-1, dict_size)\n",
        "    loss = F.cross_entropy(pred, batch_answers, ignore_index=0)\n",
        "    self.log(\"train_loss\", loss)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, _):\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy \n",
        "    pred = self.predict(batch)  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "\n",
        "    self.log(\"val_tot_accuracy\", accuracy)  # at the end of every epoch it logs the average of the accuracies of each batch\n",
        "    return accuracy\n",
        "\n",
        "  def test_step(self, batch, batch_idx, dataset_idx):\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy\n",
        "    pred = self.predict(batch.copy())  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "    \n",
        "    if (batch_idx == 0):\n",
        "      print_correct(self.idx_to_char, batch, pred)\n",
        "\n",
        "    self.log(\"test_tot_accuracy\", accuracy)  # at the end of every epoch it is logged the average of the accuracies of each batch\n",
        "    return accuracy\n",
        "\n",
        "  def predict(self, batch):  # input shape (batch_size, question_max_length)\n",
        "    batch[\"answer\"] = torch.tensor([[1] for j in range(len(batch[\"question\"]))], device=device).long()\n",
        "\n",
        "    for i in range(self.answer_max_length-1):\n",
        "      transformer_result = self(batch)\n",
        "      batch[\"answer\"] = torch.tensor([], device=device).long()\n",
        "\n",
        "      for j in range(len(transformer_result)):\n",
        "        predicted_chars = torch.argmax(transformer_result[j], 1)\n",
        "        \n",
        "        if len(predicted_chars.shape)==1:\n",
        "          predicted_chars = predicted_chars.unsqueeze(0)\n",
        "\n",
        "        start_line_char = torch.tensor([1], device=device).unsqueeze(0).long()\n",
        "        predicted_chars = torch.cat((start_line_char, predicted_chars), 1)\n",
        "\n",
        "        batch[\"answer\"] = torch.cat((batch[\"answer\"], predicted_chars), 0)\n",
        "\n",
        "    return F.one_hot(batch[\"answer\"], num_classes=self.dict_size)  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "  \n",
        "  def print_predict(self, questions):\n",
        "    pred = self.predict(questions.copy())\n",
        "    print_batch(self.idx_to_char, questions, pred)\n",
        "    return\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.995))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIr9tUDVICgD"
      },
      "source": [
        "## TP-Transformer\n",
        "The third and final model we implemented is the current state-of-the-art for the Mathematics Dataset: the Tensor-Product Transformer [2]. The architecture of this model is the same as the standard transformer except in the multi-head attention, which is replaced with a TPMHA (TP\n",
        "multi-head attention). In this model, there is a new Role vector in addition to the Key, Query, and Value vectors. We call Filler the result of the standard attention head. Each new head then binds that filler to its\n",
        "role via the tensor product and applies an affine transformation. Finally, the results of each head are summed to form the Tensor-Product Representation of the structure with multiple heads. To control dimensionality, the SOTA paper [2] suggests using pointwise multiplication, which is a contraction of the tensor product."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UlaycIzdim7"
      },
      "outputs": [],
      "source": [
        "class TPTotalEmbeddings(LightningModule):\n",
        "\n",
        "  def __init__(self, dict_size, sizes, embedding_size, padding_idx, p=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.dict_size = dict_size\n",
        "    self.max_length = max(sizes[0], sizes[1])\n",
        "    self.embedding_size = embedding_size\n",
        "    self.padding_idx = padding_idx\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    self.scale = math.sqrt(self.embedding_size)\n",
        "\n",
        "    # Layer Initialization\n",
        "    self.embedding_layer = nn.Embedding(self.dict_size, self.embedding_size, padding_idx=self.padding_idx, device=device)\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "    \n",
        "    # Sinusoidal Positional Embeddings Computation\n",
        "    self.positional_embedding = self._initialize_pos_embedding().to(device)  # shape (max_len, embedding_size)\n",
        "\n",
        "  def forward(self, input):\n",
        "    input = input.to(device)  # shape (batch_size, input_len)\n",
        "    char_embedding = self.embedding_layer(input)*self.scale\n",
        "    return self.dropout(char_embedding + self.positional_embedding[:input.shape[1], :])  # shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "\n",
        "  def _initialize_pos_embedding(self):\n",
        "    positional_embeddings = torch.zeros(self.max_length, self.embedding_size)  # shape (max_len, embedding_size)\n",
        "    for pos in range(self.max_length):\n",
        "      for i in range(0, self.embedding_size, 2):\n",
        "        positional_embeddings[pos, i] = math.sin(pos / (10000 ** (i/self.embedding_size)))\n",
        "        positional_embeddings[pos, i + 1] = math.cos(pos / (10000 ** (i/self.embedding_size)))\n",
        "    return positional_embeddings  # shape (max_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynkjd6vVdllj"
      },
      "outputs": [],
      "source": [
        "class TPPositionwiseFFN(LightningModule):\n",
        "\n",
        "  def __init__(self, embedding_size, hidden_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_units = hidden_units\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.linear_1 = nn.Linear(self.embedding_size, self.hidden_units, device=device)\n",
        "    self.linear_2 = nn.Linear(self.hidden_units, self.embedding_size, device=device)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    output = self.linear_1(input)  # shape (batch_size, input_len, hidden_units)\n",
        "    output = self.relu(output)\n",
        "    output = self.linear_2(output)  # shape (batch_size, input_len, embedding_size)\n",
        "    return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X30ASjjcc01g"
      },
      "outputs": [],
      "source": [
        "class TPMultiHeadAttention(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, masked=False):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.masked = masked\n",
        "    self.dk = self.embedding_size//self.num_heads\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.attention_heads = nn.ModuleList([TPAttentionHead(self.dk, self.embedding_size, self.masked) for _ in range(self.num_heads)])\n",
        "  \n",
        "  def forward(self, batch_keys, batch_queries, batch_values, padding_mask):  # input shape (batch_size, len_k/len_q/len_v, embedding_size)\n",
        "    \n",
        "    attention_heads_results = []\n",
        "    for head in self.attention_heads:\n",
        "      attention_heads_results.append(head(batch_keys, batch_queries, batch_values, padding_mask))  # shape (batch_size, len_q, embedding_size)\n",
        "    \n",
        "    return torch.sum(torch.stack(attention_heads_results), dim=0)  # shape (batch_size, len_q, embedding_size)\n",
        "\n",
        "\n",
        "# We use the same notation as Vaswani et al meaning that dk are the dimensions in which we project the input to create the keys and values\n",
        "class TPAttentionHead(LightningModule):\n",
        "  \n",
        "  def __init__(self, dk, embedding_size, masked):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.dk = dk\n",
        "    self.masked = masked\n",
        "    self.embedding_size = embedding_size\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.Wk = nn.Linear(embedding_size, dk, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wq = nn.Linear(embedding_size, dk, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wv = nn.Linear(embedding_size, dk, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wr = nn.Linear(embedding_size, dk, device=device)  # shape (embedding_size, dk)\n",
        "    self.Wo = nn.Linear(dk, embedding_size, device=device)  # shape (dk, embedding_size)\n",
        "  \n",
        "  def forward(self, batch_keys, batch_queries, batch_values, padding_mask):  # input shape (batch_size, len_k/len_q/len_v, embedding_size)\n",
        "    \n",
        "    keys = self.Wk(batch_keys)  # shape (batch_size, len_k, dk)\n",
        "    queries = self.Wq(batch_queries)  # shape (batch_size, len_q, dk)\n",
        "    values = self.Wv(batch_values)  # shape (batch_size, len_v, dk)\n",
        "    roles = self.Wr(batch_queries)  # shape (batch_size, len_q, dk)\n",
        "\n",
        "    unnormalized_attention_score = torch.matmul(queries, keys.transpose(1, 2)) / math.sqrt(self.dk)  # shape (batch_size, len_q, len_k)\n",
        "    \n",
        "    if self.masked:\n",
        "      input_len_1 = batch_queries.shape[1]\n",
        "      input_len_2 = batch_keys.shape[1]\n",
        "      target_mask = torch.triu(torch.full((input_len_1, input_len_2), float(\"-inf\"), device= device), diagonal=1)\n",
        "      unnormalized_attention_score = unnormalized_attention_score + target_mask\n",
        "    \n",
        "    # padding_mask shape (batch_size, len_k)\n",
        "    padding_mask = padding_mask.unsqueeze(1).repeat(1, batch_queries.shape[1], 1)  # shape (batch_size, len_q, len_k)\n",
        "    unnormalized_attention_score = unnormalized_attention_score.masked_fill_(padding_mask, float(\"-inf\"))\n",
        "\n",
        "    attention_score = F.softmax(unnormalized_attention_score, 2)\n",
        "    filler = torch.matmul(attention_score, values)  # shape (batch_size, len_q, dk)\n",
        "\n",
        "    return self.Wo(filler * roles)  # Hadamard Product # shape (batch_size, len_q, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxr0OJ8pdrPi"
      },
      "outputs": [],
      "source": [
        "class TPDecoder(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, num_layers, p=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TPDecoderLayer(self.embedding_size, self.num_heads, self.hidden_units, p=self.dropout_probability) for _ in range(self.num_layers)])\n",
        "\n",
        "  def forward(self, encoder_batch, outputs_batch, src_padding_mask, tgt_padding_mask):\n",
        "    # encoder_batch shape (batch_size, encoder_len, embedding_size)\n",
        "    # outputs_batch shape (batch_size, outputs_len, embedding_size)\n",
        "    \n",
        "    decoder_layer_ouput = outputs_batch\n",
        "\n",
        "    for decoder_layer in self.layers:\n",
        "      decoder_layer_ouput = decoder_layer(encoder_batch, decoder_layer_ouput, src_padding_mask, tgt_padding_mask)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    \n",
        "    return decoder_layer_ouput  # shape (batch_size, outputs_len, embedding_size)\n",
        "\n",
        "\n",
        "class TPDecoderLayer(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.ffn = TPPositionwiseFFN(self.embedding_size, self.hidden_units)\n",
        "\n",
        "    self.mha = TPMultiHeadAttention(self.embedding_size, self.num_heads)\n",
        "    self.masked_mha = TPMultiHeadAttention(self.embedding_size, self.num_heads, masked=True)\n",
        "    \n",
        "    self.layer_norm_1 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_2 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_3 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "  \n",
        "  def forward(self, encoder_batch, outputs_batch, src_padding_mask, tgt_padding_mask):\n",
        "    # encoder_batch shape (batch_size, encoder_len, embedding_size)\n",
        "    # outputs_batch shape (batch_size, outputs_len, embedding_size)\n",
        "\n",
        "    masked_mha_output = self.masked_mha(outputs_batch, outputs_batch, outputs_batch, tgt_padding_mask)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    masked_mha_output = self.dropout(masked_mha_output)\n",
        "    masked_mha_output = self.layer_norm_1(masked_mha_output + outputs_batch)  # add & norm\n",
        "\n",
        "    mha_output = self.mha(encoder_batch, masked_mha_output, encoder_batch, src_padding_mask)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    mha_output = self.dropout(mha_output)\n",
        "    mha_output = self.layer_norm_2(mha_output + masked_mha_output)  # add & norm\n",
        "\n",
        "    ffn_output = self.ffn(mha_output)  # shape (batch_size, outputs_len, embedding_size)\n",
        "    ffn_output = self.dropout(ffn_output)\n",
        "    ffn_output = self.layer_norm_3(ffn_output + mha_output)  # add & norm\n",
        "\n",
        "    return ffn_output   # shape (batch_size, outputs_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FymGdx8Zdswt"
      },
      "outputs": [],
      "source": [
        "class TPEncoder(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, num_layers, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.layers = nn.ModuleList(\n",
        "        [TPEncoderLayer(self.embedding_size, self.num_heads, self.hidden_units, p=self.dropout_probability) for _ in range(self.num_layers)])\n",
        "\n",
        "  def forward(self, batch, src_padding_mask):  # input shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "    encoder_layer_output = batch\n",
        "\n",
        "    for encoder_layer in self.layers:\n",
        "      encoder_layer_output = encoder_layer(encoder_layer_output, src_padding_mask)  # shape (batch_size, input_len, embedding_size)\n",
        "    \n",
        "    return encoder_layer_output  # shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "class TPEncoderLayer(LightningModule):\n",
        "  \n",
        "  def __init__(self, embedding_size, num_heads, hidden_units, p=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_units = hidden_units\n",
        "    self.dropout_probability = p\n",
        "\n",
        "    # Layers Initialization\n",
        "    self.ffn = TPPositionwiseFFN(self.embedding_size, self.hidden_units)\n",
        "    self.mha = TPMultiHeadAttention(self.embedding_size, self.num_heads)\n",
        "    \n",
        "    self.layer_norm_1 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "    self.layer_norm_2 = nn.LayerNorm(self.embedding_size, device=device)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=self.dropout_probability)\n",
        "  \n",
        "  def forward(self, batch, src_padding_mask):  # input shape (batch_size, input_len, embedding_size)\n",
        "\n",
        "    mha_output = self.mha(batch, batch, batch, src_padding_mask)  # shape (batch_size, input_len, embedding_size)\n",
        "    mha_output = self.dropout(mha_output)\n",
        "    mha_output = self.layer_norm_1(mha_output + batch)  # add & norm\n",
        "\n",
        "    ffn_output = self.ffn(mha_output)  # shape (batch_size, input_len, embedding_size)\n",
        "    ffn_output = self.dropout(ffn_output)\n",
        "    ffn_output = self.layer_norm_2(ffn_output + mha_output)  # add & norm\n",
        "\n",
        "    return ffn_output  # shape (batch_size, input_len, embedding_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlwPJOa4dvPh"
      },
      "outputs": [],
      "source": [
        "class TPTransformer(LightningModule):\n",
        "  \n",
        "  def __init__(self, dict_size, sizes, dictionaries, \n",
        "               embedding_size = 512, hidden_units = 2048, \n",
        "               num_heads = 8, num_encoder_layers=6, \n",
        "               num_decoder_layers= 6, padding_idx=0, p=0.):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    # Variables Assignment\n",
        "    self.dict_size = dict_size\n",
        "    self.question_max_length = sizes[0]\n",
        "    self.answer_max_length = sizes[1]\n",
        "    self.max_len = max(sizes[0], sizes[1])\n",
        "    self.padding_idx = padding_idx\n",
        "    self.idx_to_char = dictionaries[1]  # dictionary index to char\n",
        "    self.dropout_probability = p\n",
        "    \n",
        "    self.embedding_size = embedding_size\n",
        "    self.hidden_units = hidden_units\n",
        "    self.num_heads = num_heads\n",
        "    self.num_encoder_layers = num_encoder_layers\n",
        "    self.num_decoder_layers = num_decoder_layers\n",
        "    \n",
        "    # Layers Initialization\n",
        "    self.total_embedding = TPTotalEmbeddings(self.dict_size, sizes, self.embedding_size, self.padding_idx, p=self.dropout_probability)\n",
        "\n",
        "    self.encoder = TPEncoder(self.embedding_size, num_heads= self.num_heads, \n",
        "                           hidden_units=self.hidden_units, num_layers=self.num_encoder_layers, p=self.dropout_probability)\n",
        "    \n",
        "    self.decoder = TPDecoder(self.embedding_size, num_heads= self.num_heads, \n",
        "                           hidden_units=self.hidden_units, num_layers=self.num_decoder_layers, p=self.dropout_probability)\n",
        "    \n",
        "  def forward(self, batch):\n",
        "    \n",
        "    questions = (batch[\"question\"]).to(device)  # shape (batch_size, question_max_length)\n",
        "    answers = (batch[\"answer\"]).to(device)  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    source_pad_mask = (questions == 0).to(device)  # shape (batch_size, question_max_length)\n",
        "    target_pad_mask = (answers == 0).to(device)  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    embedded_questions = self.total_embedding(questions)  # shape (batch_size, question_max_length, embedding_size)\n",
        "    embedded_answers = self.total_embedding(answers)  # shape (batch_size, answer_max_length-1, embedding_size)\n",
        "\n",
        "    encoder_output = self.encoder(embedded_questions, src_padding_mask=source_pad_mask)  # shape (batch_size, question_max_length, embedding_size)\n",
        "    decoder_output = self.decoder(encoder_output, embedded_answers, src_padding_mask=source_pad_mask, tgt_padding_mask=target_pad_mask)  # shape (batch_size, answer_max_length-1, embedding_size)\n",
        "\n",
        "    # Softmax not used because of nn.CrossEntropyLoss\n",
        "    return torch.matmul(decoder_output, torch.transpose(self.total_embedding.embedding_layer.weight, 0, 1))  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "\n",
        "  def training_step(self, batch, _):\n",
        "    # Preparing inputs  \n",
        "    batch_answers = batch[\"answer\"][:, 1:].flatten(0, 1)  # shape (batch_size * answer_max_length-1)\n",
        "    batch[\"answer\"] = batch[\"answer\"][:, :-1]  # shape (batch_size, answer_max_length-1)\n",
        "\n",
        "    # Computing prediction and loss\n",
        "    pred = self(batch).flatten(0, 1)  # shape (batch_size * answer_max_length-1, dict_size)\n",
        "    loss = F.cross_entropy(pred, batch_answers, ignore_index=0)\n",
        "    self.log(\"train_loss\", loss)\n",
        "    return loss\n",
        "\n",
        "  def validation_step(self, batch, _):\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy \n",
        "    pred = self.predict(batch)  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "\n",
        "    self.log(\"val_tot_accuracy\", accuracy)  # at the end of every epoch it logs the average of the accuracies of each batch\n",
        "    return accuracy\n",
        "\n",
        "  def test_step(self, batch, batch_idx, dataset_idx):\n",
        "    batch_answers = batch[\"answer\"]  # shape (batch_size, answer_max_length)\n",
        "\n",
        "    # Computing prediction and accuracy\n",
        "    pred = self.predict(batch.copy())  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "    accuracy = paper_accuracy(pred, batch_answers)  # accuracy for the current batch as defined in the \"mathematics dataset\" paper\n",
        "    \n",
        "    if (batch_idx == 0):\n",
        "      print_correct(self.idx_to_char, batch, pred)\n",
        "\n",
        "    self.log(\"test_tot_accuracy\", accuracy)  # at the end of every epoch it is logged the average of the accuracies of each batch\n",
        "    return accuracy\n",
        "\n",
        "  def predict(self, batch):  # shape (batch_size, question_max_length)\n",
        "    batch[\"answer\"] = torch.tensor([[1] for j in range(len(batch[\"question\"]))], device=device).long()\n",
        "\n",
        "    for i in range(self.answer_max_length-1):\n",
        "      transformer_result = self(batch)\n",
        "      batch[\"answer\"] = torch.tensor([], device=device).long()\n",
        "\n",
        "      for j in range(len(transformer_result)):\n",
        "        predicted_chars = torch.argmax(transformer_result[j], 1)\n",
        "        \n",
        "        if len(predicted_chars.shape)==1:\n",
        "          predicted_chars = predicted_chars.unsqueeze(0)\n",
        "\n",
        "        start_line_char = torch.tensor([1], device=device).unsqueeze(0).long()\n",
        "        predicted_chars = torch.cat((start_line_char, predicted_chars), 1)\n",
        "\n",
        "        batch[\"answer\"] = torch.cat((batch[\"answer\"], predicted_chars), 0)\n",
        "\n",
        "    return F.one_hot(batch[\"answer\"], num_classes=self.dict_size)  # shape (batch_size, answer_max_length-1, dict_size)\n",
        "  \n",
        "  def print_predict(self, questions):\n",
        "    pred = self.predict(questions.copy())\n",
        "    print_batch(self.idx_to_char, questions, pred)\n",
        "    return\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=1e-4, betas=(0.9, 0.995))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay9IfT_WgJ_5"
      },
      "source": [
        "# Train a model\n",
        "This section contains the code to train a model.\n",
        "\n",
        "For this section to work properly, you must run the following sections:\n",
        "- Imports\n",
        "- Dataset and Preprocessing\n",
        "- Model definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L9mXV_fiPFp"
      },
      "source": [
        "## LSTM\n",
        "We chose a subset of the dataset composed by three modules (numbers round, calculus differentiate, polynomials evaluate) for a total of\n",
        "6 million samples.\n",
        "\n",
        "We trained this model for 10 epochs using the Adam optimizer, the cross-entropy loss function, a batch size of 1024, a learning rate of 6e−4 and other hyperparameters as suggested in the dataset paper [1].\n",
        "We obtained an accuracy of 0.560 for the interpolation test set and 0.806 for the extrapolation test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMuyX1PCiimF"
      },
      "outputs": [],
      "source": [
        "# Initialize the dataset\n",
        "mdm = MathDataModule(chosen_dataset=0, batch_size=256)\n",
        "mdm.prepare_data()\n",
        "mdm.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwLXnHJNiimG"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = LSTM(mdm.get_dictionary_size(), mdm.get_max_lengths(), mdm.get_dictionaries())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4llFp0iimG"
      },
      "outputs": [],
      "source": [
        "# Set up logger\n",
        "wandb.login()\n",
        "wandb.init(project=\"DL\")\n",
        "wandb.watch(model, log_freq=10)\n",
        "wandb_logger = WandbLogger(project=\"DL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU5ViZpXiimG"
      },
      "outputs": [],
      "source": [
        "# Initialize the trainer\n",
        "trainer = Trainer(max_epochs=10, accelerator=\"gpu\", logger=wandb_logger, log_every_n_steps=10)\n",
        "\n",
        "# Training the model\n",
        "trainer.fit(model, train_dataloaders=mdm.train_dataloader(), val_dataloaders=mdm.val_dataloader())\n",
        "\n",
        "# Stop logging\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lSVbbjUtrYg"
      },
      "outputs": [],
      "source": [
        "trainer.save_checkpoint(\"/content/LSTM.ckpt\")\n",
        "# Remember to save also the dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvvFMOvhiTp0"
      },
      "source": [
        "## Transformer\n",
        "We chose a subset of the dataset composed by three modules (numbers round, calculus differentiate, polynomials evaluate) for a total of 6 million samples.\n",
        "\n",
        "We trained this model for 3 epochs using the Adam optimizer, the cross-entropy loss function, a batch size of 512, a learning rate of 1e−4 and other hyperparameters as suggested in the dataset paper [1]. We obtained an accuracy of 0.663 for the interpolation test set and 0.913 for the extrapolation test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_yxJ0Qzijk_"
      },
      "outputs": [],
      "source": [
        "# Initialize the dataset \n",
        "mdm = MathDataModule(chosen_dataset=0, batch_size=256)\n",
        "mdm.prepare_data()\n",
        "mdm.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8keR4MZijlA"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = Transformer(mdm.get_dictionary_size(), mdm.get_max_lengths(), mdm.get_dictionaries(), p=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7zKRkT1ijlA"
      },
      "outputs": [],
      "source": [
        "# Set up logger\n",
        "wandb.login()\n",
        "wandb.init(project=\"DL\")\n",
        "wandb.watch(model, log_freq=10)\n",
        "wandb_logger = WandbLogger(project=\"DL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGTmBJwTijlA"
      },
      "outputs": [],
      "source": [
        "# Initialize the trainer\n",
        "trainer = Trainer(max_epochs=3, accelerator=\"gpu\", logger=wandb_logger, log_every_n_steps=10)\n",
        "\n",
        "# Training the model \n",
        "trainer.fit(model, train_dataloaders=mdm.train_dataloader(), val_dataloaders=mdm.val_dataloader())\n",
        "\n",
        "# Stop logging\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks5U_tk9ijlB"
      },
      "outputs": [],
      "source": [
        "trainer.save_checkpoint(\"/content/TRANSFORMER.ckpt\")\n",
        "# Remember to save also the dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy46poFliWXn"
      },
      "source": [
        "## TP-Transformer\n",
        "We chose a subset of the dataset composed by three modules (numbers round, calculus differentiate, polynomials evaluate) for a total of 6 million samples.\n",
        "\n",
        "We trained this model for 3 epochs using the Adam optimizer, the cross-entropy loss function, a batch size of 512, a learning rate of 1e−4 and other hyperparameters as suggested in the SOTA paper [2] (embedding size 256 and dimension of the feedforward networks 2048). We obtained an accuracy of 0.0.684 for the interpolation test set and 0.957 for the extrapolation test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnkyFKNCikSf"
      },
      "outputs": [],
      "source": [
        "# Initialize the dataset \n",
        "mdm = MathDataModule(chosen_dataset=0, batch_size=256)\n",
        "mdm.prepare_data()\n",
        "mdm.setup(\"fit\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VN7cJRlikSg"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = TPTransformer(mdm.get_dictionary_size(), mdm.get_max_lengths(), mdm.get_dictionaries(), p=0.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5tf9f1OikSg"
      },
      "outputs": [],
      "source": [
        "# Set up logger \n",
        "wandb.login()\n",
        "wandb.init(project=\"DL\")\n",
        "wandb.watch(model, log_freq=10)\n",
        "wandb_logger = WandbLogger(project=\"DL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4StA2N4_ikSh"
      },
      "outputs": [],
      "source": [
        "# Initialize the trainer\n",
        "trainer = Trainer(max_epochs=3, accelerator=\"gpu\", logger=wandb_logger, log_every_n_steps=10)\n",
        "\n",
        "# Training the model\n",
        "trainer.fit(model, train_dataloaders=mdm.train_dataloader(), val_dataloaders=mdm.val_dataloader())\n",
        "\n",
        "# Stop logging\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRxtHJYzuIws"
      },
      "outputs": [],
      "source": [
        "trainer.save_checkpoint(\"/content/TPTRANSFORMER.ckpt\")\n",
        "# Remember to save also the dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V52kHa5EzMiX"
      },
      "source": [
        "# Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV4WNbG1rjVN"
      },
      "outputs": [],
      "source": [
        "!gdown 1k2TzLZIyHJSPO2M8UQFIgcvN2gtLpKVF -O /content/LSTM.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gagO68_yrytm"
      },
      "outputs": [],
      "source": [
        "!gdown 1_Pk9mCA5CvdV68u0saoqKaTBpnTNuEOK -O /content/TRANSFORMER.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKqjAiQyr2Jo"
      },
      "outputs": [],
      "source": [
        "!gdown 1lM9UrljBcFH7ZUS6BfoVLHIF6CtTtLLI -O /content/TPTRANSFORMER.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4dShLdAvdLh"
      },
      "source": [
        "#Import a pretrained model and test it\n",
        "This section contains the code to test a pretrained model.\n",
        "\n",
        "For this section to work properly, you must run the following sections:\n",
        "- Imports\n",
        "- Dataset and Preprocessing\n",
        "- Model definitions\n",
        "- Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtlTAvwIwE6-"
      },
      "outputs": [],
      "source": [
        "mdm = MathDataModule(chosen_dataset=4)\n",
        "mdm.prepare_data()\n",
        "mdm.question_max_length = 162\n",
        "mdm.answer_max_length = 32\n",
        "mdm.char_to_idx = {'&': 0, '#': 1, '@': 2, '{': 3, '\"': 4, 'q': 5, 'u': 6, 'e': 7, 's': 8, 't': 9, 'i': 10, 'o': 11, 'n': 12, ':': 13, ' ': 14, 'R': 15, 'd': 16, '-': 17, '0': 18, '.': 19, '7': 20, '1': 21, '4': 22, '5': 23, 'p': 24, ',': 25, 'a': 26, 'w': 27, 'r': 28, '}': 29, '6': 30, '8': 31, '2': 32, 'h': 33, 'W': 34, 'c': 35, 'm': 36, 'l': 37, '?': 38, '9': 39, '3': 40, 'z': 41, 'f': 42, 'v': 43, 'x': 44, 'g': 45, 'L': 46, '(': 47, ')': 48, '=': 49, '*': 50, 'D': 51, '+': 52, 'G': 53, 'b': 54, 'y': 55, 'C': 56, 'j': 57, 'k': 58, 'F': 59}\n",
        "mdm.idx_to_char = {0: '&', 1: '#', 2: '@', 3: '{', 4: '\"', 5: 'q', 6: 'u', 7: 'e', 8: 's', 9: 't', 10: 'i', 11: 'o', 12: 'n', 13: ':', 14: ' ', 15: 'R', 16: 'd', 17: '-', 18: '0', 19: '.', 20: '7', 21: '1', 22: '4', 23: '5', 24: 'p', 25: ',', 26: 'a', 27: 'w', 28: 'r', 29: '}', 30: '6', 31: '8', 32: '2', 33: 'h', 34: 'W', 35: 'c', 36: 'm', 37: 'l', 38: '?', 39: '9', 40: '3', 41: 'z', 42: 'f', 43: 'v', 44: 'x', 45: 'g', 46: 'L', 47: '(', 48: ')', 49: '=', 50: '*', 51: 'D', 52: '+', 53: 'G', 54: 'b', 55: 'y', 56: 'C', 57: 'j', 58: 'k', 59: 'F'}\n",
        "mdm.setup(\"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTtgIJQEvhI2"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "model = LSTM.load_from_checkpoint(\"/content/LSTM.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())\n",
        "trainer = Trainer(accelerator=\"gpu\")\n",
        "trainer.test(model, dataloaders=mdm.test_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3csOt55wbA_"
      },
      "outputs": [],
      "source": [
        "#Transformer\n",
        "model = Transformer.load_from_checkpoint(\"/content/TRANSFORMER.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())\n",
        "trainer = Trainer(accelerator=\"gpu\")\n",
        "trainer.test(model, dataloaders=mdm.test_dataloader())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIF1KYw1wbRr"
      },
      "outputs": [],
      "source": [
        "#TP-Transformer\n",
        "model = TPTransformer.load_from_checkpoint(\"/content/TPTRANSFORMER.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())\n",
        "trainer = Trainer(accelerator=\"gpu\")\n",
        "trainer.test(model, dataloaders=mdm.test_dataloader())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7abGcEtguWHo"
      },
      "source": [
        "# Import a pretrained model and predict\n",
        "This section contains the code to make a prediction using a pretrained model.\n",
        "\n",
        "For this section to work properly, you must run the following sections:\n",
        "- Imports\n",
        "- Dataset and Preprocessing\n",
        "- Model definitions\n",
        "- Download pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5j-oDuRuioH"
      },
      "outputs": [],
      "source": [
        "mdm = MathDataModule()\n",
        "mdm.question_max_length = 162\n",
        "mdm.answer_max_length = 32\n",
        "mdm.char_to_idx = {'&': 0, '#': 1, '@': 2, '{': 3, '\"': 4, 'q': 5, 'u': 6, 'e': 7, 's': 8, 't': 9, 'i': 10, 'o': 11, 'n': 12, ':': 13, ' ': 14, 'R': 15, 'd': 16, '-': 17, '0': 18, '.': 19, '7': 20, '1': 21, '4': 22, '5': 23, 'p': 24, ',': 25, 'a': 26, 'w': 27, 'r': 28, '}': 29, '6': 30, '8': 31, '2': 32, 'h': 33, 'W': 34, 'c': 35, 'm': 36, 'l': 37, '?': 38, '9': 39, '3': 40, 'z': 41, 'f': 42, 'v': 43, 'x': 44, 'g': 45, 'L': 46, '(': 47, ')': 48, '=': 49, '*': 50, 'D': 51, '+': 52, 'G': 53, 'b': 54, 'y': 55, 'C': 56, 'j': 57, 'k': 58, 'F': 59}\n",
        "mdm.idx_to_char = {0: '&', 1: '#', 2: '@', 3: '{', 4: '\"', 5: 'q', 6: 'u', 7: 'e', 8: 's', 9: 't', 10: 'i', 11: 'o', 12: 'n', 13: ':', 14: ' ', 15: 'R', 16: 'd', 17: '-', 18: '0', 19: '.', 20: '7', 21: '1', 22: '4', 23: '5', 24: 'p', 25: ',', 26: 'a', 27: 'w', 28: 'r', 29: '}', 30: '6', 31: '8', 32: '2', 33: 'h', 34: 'W', 35: 'c', 36: 'm', 37: 'l', 38: '?', 39: '9', 40: '3', 41: 'z', 42: 'f', 43: 'v', 44: 'x', 45: 'g', 46: 'L', 47: '(', 48: ')', 49: '=', 50: '*', 51: 'D', 52: '+', 53: 'G', 54: 'b', 55: 'y', 56: 'C', 57: 'j', 58: 'k', 59: 'F'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH0kTzzxukjl"
      },
      "outputs": [],
      "source": [
        "#LSTM\n",
        "model = LSTM.load_from_checkpoint(\"/content/LSTM.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhati0frw-2y"
      },
      "outputs": [],
      "source": [
        "#Transformer\n",
        "model = Transformer.load_from_checkpoint(\"/content/TRANSFORMER.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "imyzd3xDw--h"
      },
      "outputs": [],
      "source": [
        "#TP-Transformer\n",
        "model = TPTransformer.load_from_checkpoint(\"/content/TPTRANSFORMER.ckpt\", dict_size=mdm.get_dictionary_size(), sizes=mdm.get_max_lengths(), dictionaries=mdm.get_dictionaries())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QkmG1r5likSh"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-G5ShEbikSh"
      },
      "outputs": [],
      "source": [
        "#To test a new question add it to the list\n",
        "questions = []\n",
        "questions.append(\"Let t(x) = -x**2 + 3*x - 3. Calculate t(3).\")  # correct: -3\n",
        "questions.append(\"What is -0.0006832 rounded to 5 decimal places?\")  # correct: -0.00068\n",
        "questions.append(\"Find the first derivative of 2*d**4 - 35*d**2 - 695 wrt d.\")  # correct: 8*d**3 - 70*d\n",
        "\n",
        "for question in questions:\n",
        "  encoded = mdm.encode_question(question)\n",
        "  model.print_predict(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_A4kEE0pTyJ"
      },
      "source": [
        "#Conclusions\n",
        "Looking at the experimental results, we found that the Transformer and TP-Transformer models are slower in completing an epoch, but they need fewer of them to reach similar levels of accuracy with respect to the LSTM.\n",
        "Moreover, as we thought, the TP-Transformer performed better than the other models, both in the Interpolation and the Extrapolation test sets.\n",
        "However, we believe that our results are highly influenced by the lack of appropriate computational resources. We are pretty sure that we would get even better results if additional training were performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFdqPrYppay"
      },
      "source": [
        "#References\n",
        "[1] David Saxton et al. Analysing Mathematical Reasoning Abilities of Neural Models. 2019. doi: 10.48550/ARXIV.1904.01557. url: https://arxiv.org/abs/1904.01557.\n",
        "\n",
        "[2] Imanol Schlag et al. Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving. 2019. doi: 10.48550/ARXIV.1910.06611. url: https://arxiv.org/abs/1910.06611.\n",
        "\n",
        "[3] Ashish Vaswani et al. Attention Is All You Need. 2017. doi: 10.48550/ARXIV.1706.03762. url: https://arxiv.org/abs/1706.03762."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPx5O-Ko1kOE"
      },
      "source": [
        "#Appendix: WandB Report\n",
        "Launch the code cell to view the WandB report. It contains charts of the training loss and the validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54tfH5OBnO33"
      },
      "outputs": [],
      "source": [
        "%wandb project_dl/DL/reports/Deep-Learning-Project--VmlldzozNDk3Njc4 -h 720"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "45ssSUDoqHpJ",
        "lh5rcecfIyC_",
        "Xy0hBvR5Emn9",
        "XnAsuqXrIfSv",
        "eIAeNeOORtpX",
        "0-KCL0xxH84t",
        "kqNtvtuyH-dd",
        "SIr9tUDVICgD",
        "ay9IfT_WgJ_5",
        "6L9mXV_fiPFp",
        "xvvFMOvhiTp0",
        "fy46poFliWXn",
        "V52kHa5EzMiX",
        "S4dShLdAvdLh",
        "7abGcEtguWHo",
        "DPx5O-Ko1kOE"
      ],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.2 (default, Apr  8 2021, 23:19:18) \n[Clang 12.0.5 (clang-1205.0.22.9)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
